1.DL4J准备：
	jdk7以上
	maven3.3以上

2.安装
	进入dl4j-examples目录
	mvn compile命令会在根目录生成target文件
	mvn clean可将根目录下生成的target文件移除
	mvn install的时候会先执行mvn package，maven就是通过这个生命周期来根据用户配置，进行打包（war、jar或者其他）

3.idea导入项目
	import project
	选中：dl4j-examples-master
	选择import project from external model,选中Maven
	之后默认.

4.神经网络术语
输入层/输出层/隐藏层——顾名思义，输入层是接收输入信号的一层，也是该网络的第一层；输出层则是传递输出信号的一层，也是该网络的最后一层。
Batches：当我们训练一个神经网路时，我们不应一次性发送全部输入信号，而应把输入信号随机分成几个大小相同的数据块发送。
周期(Epochs)一个周期表示对所有的数据批次都进行了一次迭代，包括一次正向传播和一次反向传播，所以一个周期就意味着对所有的输入数据分别进行一次正向传播和反向传播。
为什么要使用多于一个 epoch？在神经网络中传递完整的数据集一次是不够的，而且我们需要将完整的数据集在同样的神经网络中传递多次。但是请记住，我们使用的是有限的数据集，并且我们使用一个迭代过程即梯度下降，优化学习。因此仅仅更新权重一次或者说使用一个 epoch 是不够的。
深度学习框架的后端是什么？
高度封装的API,与不高度封装的区别

5.神经网络中怎么确定节点数
输入向量维数=输入层节点数
输出向量维数=输出层节点数

6.BP神经网络
一种按照误差逆向传播算法训练的多层前馈神经网络
BP算法：
BP算法是由学习过程由信号的正向传播与误差的反向传播两个过程组成。由于多层前馈网络的训练经常采用误差反向传播算法，人们也常把将多层前馈网络直接称为BP网络。

7.mnist文件格式
mnist文件格式是idx，是一种特殊的二进制文件，用来存储向量与多维矩阵的文件格式。

8.监督训练的过程
把一个输入模式送给网络，考察它这时的输出，并将它的输出与目标输出进行比较。如果实际的输出和目标输出不同，则网络所有的权重都要作稍微的改变，使得下一次再用同样模式输入时，网络的输出能和期望的结果靠近一点。这一过程必须对要求网络学习的每一种输入模式进行重复，直到网络能正确识别每一种模式为止。
反向传播调整权重的方式：
由输出层开始,一层一层地向输入层方向推，直到到达第一个隐藏层为止，使所有层的权重都获得少量的调整（需要一层一层的逐层训练）。如果这一工作正确做完，则输入模式再一次被送入时，网络的实际输出将会向目标输出靠近一点。这样的全部过程要对所有不同的输入模式重复进行许多次，直到误差值降低到所处理问题可接受的一个极限值以内。
调整输出层的权重：
通过反向传播计算出权重w1对误差的偏导数，通过梯度下降w'1 = w1-a*偏导（a为学习速率，w'1就是新的w1）
隐藏与隐藏层之间
是一样的都是对最终损失求此层中权重的偏导数，可能求偏导数要复杂些，最后也是通过梯度下降调整权重。
(所以我们真正测量的是每个参数对误差总变化的贡献的偏导数。注意都是对总误差)

9.循环网络的优化：
沿时间反向传播，或称BPTT，
截断式BPTT，截断式BPTT是完整BPTT的近似方法，也是处理较长序列时的优先选择

10.梯度弥散与膨胀
频繁乘以略大于一的数，就会增大到无法衡量的地步，梯度膨胀。频繁乘以略小于一的数，就会变为0，梯度消失.
梯度膨胀的问题相对比较容易解决，因为可以将其截断或挤压。而消失的梯度则有可能变得过小，以至于计算机无法处理，网络无法学习－这个问题更难解决。
LSTM，可以解决梯度消失的问题.

11.softsign
softsign(x)=x/(1+|x|)
Softsign 是 Tanh 激活函数的另一个替代选择。就像 Tanh 一样，Softsign 是反对称、去中心、可微分，并返回-1 和 1 之间的值。其更平坦的曲线与更慢的下降导数表明它可以更高效地学习，比tanh更好的解决梯度消失的问题

12.GravesLSTM
由Alex Graves改良的LSTM

13.RNN
所谓的循环就是层的内部的神经元之间有影响，在前馈网络中层内部没关系只有层之间有关系。

14.您有大量数据，一开始不妨将批次大小设定为1000


15.广义上讲，任何能够从事某种智能活动的计算机程序都是人工智能
深度神经网络，定义：深度神经网络则不止一个隐藏层。
只有一层的叫单层神经网络，（单层的）多层感知机

16，RNTN，递归神经张量网络

17.迁移学习的作用
一旦的模型中包含一些隐藏层时，增添多一层隐藏层将会花费巨大的计算资源。“迁移学习”的方式，可以使我们在他人训练过的模型基础上进行小改动便可投入使用。
神经网络需要用数据来训练，它从数据中获得信息，进而把它们转换成相应的权重。这些权重能够被提取出来，迁移到其他的神经网络中，我们“迁移”了这些学来的特征，就不需要从零开始训练一个神经网络了。
为了实现迁移学习，所以需要预训练模型，预训练模型(pre-trained model)是前人为了解决类似问题所创造出来的模型。你在解决问题的时候，不用从零开始训练一个新模型，可以从在类似问题中训练过的模型入手。

18.arff是Attribute-Relation File Format缩写
是weka数据挖掘开源程序使用的一种文件模式。WEKA的全名是怀卡托智能分析环境，WEKA作为一个公开的数据挖掘工作平台，集合了大量能承担数据挖掘任务的机器学习算法，包括对数据进行预处理，分类，回归、聚类、关联规则以及在新的交互式界面上的可视化。
arff开头的通用格式示例：
@relation weather  %关系名，也就是这个arff文件的名称
@attribute outlook {sunny, overcast, rainy}   %标量属性的可选类，{}加，
@attribute temperature numeric
@attribute humidity numeric
@attribute windy {true, false}
@attribute play {yes, no}
@data                           %表示数据的开始
sunny,85,85,FALSE,no
11 sunny,80,90,TRUE,no
12 overcast,83,86,FALSE,yes
13 rainy,70,96,FALSE,yes
14 rainy,68,80,FALSE,yes
15 rainy,65,70,TRUE,no
16 overcast,64,65,TRUE,yes
17 sunny,72,95,FALSE,no
18 sunny,69,70,FALSE,yes
19 rainy,75,80,FALSE,yes
20 sunny,75,70,TRUE,yes
21 overcast,72,90,TRUE,yes
22 overcast,81,75,FALSE,yes
23 rainy,71,91,TRUE,no


19.DataSetIterator是用于遍历列表元素的一个Deeplearning4J类
ImageRecordReader是RecordReader的子类
RecordReader是DataVec中的一个类

20.RDD(Resilient Distributed Datasets),弹性分布式数据集

21.康威定理：
组织沟通方式会通过系统设计表达出来.(软件系统能够反映人员的组织结构)
沟通成本 = n(n-1)/2，沟通成本随着项目或者组织的人员增加呈指数级增长。项目管理这个算法的复杂度是O(n^2)
时间再多一件事情也不可能做的完美，但总有时间做完一件事情。
安全有两种方式：
	常规的安全指的是尽可能多的发现并消除错误的部分，达到绝对安全，这是理想。
	另一种则是弹性安全，即使发生错误，只要及时恢复，也能正常工作，这是现实。

22.卷积神经网络
移动的幅度称为步幅stride。
卷积核（Convolution Kernel），也叫过滤器filter
卷积网络以矩形接收正常色彩的图像。这一矩形的宽度和高度由其像素点进行衡量，深度则包含三层，每层代表RGB中的一个字母。这些深度层被称为通道。


23.语义哈希(semanticHashing)是指将高维空间向量映射至低维汉明空间,并保持原空间向量相似性,使得新空间向量的汉明距离反映原空间向量相似度的哈希算法。

24.玻尔兹曼机
使用了玻尔兹曼分布作为激活函数，所以称为玻尔兹曼机，其原理其实是模拟退火算法

25.深度自动编码器
new RBM.Builder()

26.降噪自动编码器
为了防止过拟合问题而对输入的数据（网络的输入层）加入噪音，使学习得到的编码器W具有较强的鲁棒性，从而增强模型的泛化能力.
堆叠式降噪自动编码器（SDA）与降噪自动编码器的关系就像是深度置信网络与受限玻尔兹曼机一样.
堆叠自动编码机（SdA）是由一系列去噪自动编码机堆叠而成，每个去噪自动编码机的中间层(即编码层)作为下一层的输入层，这样一层一层堆叠起来，构成一个深层网络，这些网络组成堆叠去噪自动编码机（SdA）的表示部分。
SDA的关键功能之一是随着输入的传递逐层进行无监督预定型，更广义地来看，这也是深度学习的关键功能之一。当每个层都接受了预定型，学习了如何对来自前一层的输入进行特征选择和提取之后，就可以开始第二阶段的有监督微调。

27.硬饱和与软饱和
对于任意的x，如果存在常数c，当x<c时恒有f'(x)=0 称为左硬饱和，当x>c时恒有f'(x)=0 则称为右硬饱和，两种条件都满足时称为硬饱和；如果只有在极限状态下偏导数等于0的函数，称之为软饱和。

28.横向扩展Scale Out
Scale Out（横向扩展）向外扩展，指的是采购新的设备，和现有设备一起提供更强的负载能力。

29.倒排索引
倒排索引源于实际应用中需要根据属性的值来查找记录。
文档和词条之间的关系是正向索引。
倒排索引存储的是分词（Term）和文档（Doc）之间的关系。


30.本征
本征向量（eigenvector）一词中的“本征（eigen）”来自德语，原意为 "自己的"。所以 “本征” 表示了两件事物之间的一种特殊关系。这是一种特定、独特而明确的关系：这辆车或这个向量是我的，而不是别人的。

31.信息增益
获得信息，降低熵——这就是信息增益

32.ieee 754
单精度，符号位：1，阶码：8，尾数：23
双精度，符号位：1，阶码：11，尾数：52

33.状态空间
所有可能状态的列表称为“状态空间”。您拥有的状态越多，空间越大，组合问题就越复杂。

34.每个微批次只有一个分类类别,可能会导致分值与迭代次数关系图出现很大波动或异常形态.

35.“relu”或“leakyrelu”激活函数一般是比较好的选择。其他一些激活函数（tanh、sigmoid等）更容易出现梯度消失问题，进而大幅增加深度神经网络学习的难度。但是，LSTM层仍然普遍使用tanh激活函数。

36.常见的微批次大小范围在16～128之间

37.隐藏层设置规则
任何隐藏层的节点数均不应少于输入层的四分之一。输出层的节点数应等于标签数。
许多较小的数据集可能只需要三到四个隐藏层，再加大深度会导致准确率下降。
较大的数据集需要更多隐藏层。
一般的规则是：数据集越大，差异也越多，网络需要更多的特征/神经元才能获得准确的结果。

38.Bag of Words（BoW）是一种计算单词出现在文档中的次数的算法。

39.深度自动编码器
深度自动编码器由两个对称的深度置信网络组成，这些网络通常具有四个或五个浅层，表示网络的编码一半，第二组四个或五个层构成解码半部分。
深度置信网络DBN的基本结构为RBM。
作用：图片搜索，数据压缩，主题建模与信息检索（IR）

40.语义哈希(semanticHashing)是指将高维空间向量映射至低维汉明空间,并保持原空间向量相似性,使得新空间向量的汉明距离反映原空间向量相似度的哈希算法。语义哈希引入了近似的概念,认为在海量数据的搜索中,在大多数情况下,完全精确的查找并不是必须的,近似解己经足以满足用户绝大多数的要求,因而通过哈希算法迅速定位数据集中一定概率下与搜索关键词相关的数据,配合汉明空间相似度度量的快速性和索引结果容易进一步扩展的特点,可以大幅提高索引和检索的效率。

41.当我们无法确定什么是真的时，我们应该根据最可能的事情采取行动。笛卡尔 

42.强化学习
强化学习是一种机器学习，其中代理人通过对这些动作的结果执行某些动作和学习来学习如何在环境中行为。代理人采取行动时，根据结果得到报酬。这样，学习过程继续取决于积极和消极的回报。
智能体与环境进行不断地交互从而产生很多数据。强化学习算法利用产生的数据修改自身的动作策略，再与环境交互，产生新的数据，并利用新的数据进一步改善自身的行为，经过数次迭代学习后，智能体能最终地学到完成相应任务的最优动作（最优策略）。这就是一个强化学习的过程。
强化学习实现的途径：
基于价值的
基于策略的
基于模型的。
组成：
代理人agent
行动A
贴现因子gamma：γ表示
环境
状态S
奖励R
策略π
价值V
Q值或动作值（Q）

DQN(Deep Q-Learning)深度强化学习 deep q network


43.机器学习模型如何发布？
模型更新应独立于应用程序进程并独立扩展，以便它们松散耦合。你有一个应用程序，它通常很小，可以击中模型，通常更大，有不同的要求。

44.数据只能向后理解; 但它必须向前生活

45.ETL，是英文 Extract-Transform-Load 的缩写，用来描述将数据从来源端经过抽取（extract）、交互转换（transform）、加载（load）至目的端的过程.

46.梯度归一化有助于避免梯度过大（所谓的爆炸梯度问题，在递归神经网络中常见）或太小.

47.弱共识（又叫沉默共识），我知道，你知道，大家都知道。
强共识，所有人都知道，所有人都知道所有人都知道。

48.径向基函数
径向基函数（RBF）是一个函数，它为来自其域的每个输入分配一个实数值（它是一个实值函数），并且RBF产生的值总是一个绝对值; 即它是距离的度量，不能是负的。其功能类似激活函数。
svm的核函数：
核函数的作用就是隐含着一个从低维空间到高维空间的映射，而这个映射可以把低维空间中线性不可分的两类点变成线性可分的。
线性核函数，多项式核函数，径像基核函数/高斯核函数，拉普拉斯核函数，sigmod核函数

49.图像预处理 （1） 尺度调整：将不同大小的训练样本集图像尺寸调整为 48*48 （2） 对比度变换：将图像对比度归一化的三种方法 A. 将三个彩色空间的像素围绕平均像素强度线性变换加减一 个标准偏差。
B. 将三个彩色空间的像素围绕平均像素强度线性变换加减两 个个标准偏差。
C. Contrast-limited Adaptive Histogram Equalization (CLAHE)对比受限 的自适应直方图均衡化。
第三种对比度变换产生的效果最好。
图像扭曲： 图像的位移， 旋转度和尺度变换大小值都是在特定范围均匀分布 的，在正负 10%范围内。

50.符号推理

51.深度学习应用
flaw detection 缺陷检测
Fraud detection反欺诈侦查
Sentiment analysis 情感分析

52.RPA
机器人过程自动化.RPA和AI是两种在目标和界面上截然不同的横向技术。RPA旨在为商业和白领工人节省时间。RPA由RPA工程师通过GUI或图形界面构建，用于安排RPA自动执行的任务序列。在大多数情况下，RPA基于规则或if-then语句，告诉程序在某些条件下该做什么。
AI是一个总括性术语，包括上述类型的规则引擎。但这并不是人工智能的激动人心的一面，而这通常不是人们现在提到人工智能时的意思。通常，它们指的是机器学习或深度学习; 即能够根据他们的环境或他们所接触的数据重写自己的程序。
RPA和AI重叠，因为您可以使用AI注入RPA。大多数RPA供应商目前没有在他们发布的产品中使用高级AI，但这种情况正在发生变化。高级AI在RPA中的有用应用可包括图像识别（为了更可靠地识别屏幕上的图像）或文本分析。

53.齐普夫定律
如果把一篇较长文章中每个词出现的频次统计起来，按照高频词在前、低频词在后的递减顺序排列，并用自然数给这些词编上等级序号，即频次最高的词等级为1，频次次之的等级为2，……，频次最小的词等级为D。若用f表示频次，r表示等级序号，则有fr=C(C为常数).词频x等级=常数

54.韦布尔分布
从概率论和统计学角度看，Weibull Distribution是连续性的概率分布，其概率密度为：
其中，x是随机变量，λ>0是比例参数（scale parameter），k>0是形状参数（shape parameter）。显然，它的累积分布函数是扩展的指数分布函数，而且，Weibull distribution与很多分布都有关系。如，当k=1，它是指数分布；k=2且时，是Rayleigh distribution（瑞利分布）。
    威布尔分布在可靠性工程中被广泛应用，尤其适用于机电类产品的磨损累计失效的分布形式。由于它可以利用概率值很容易地推断出它的分布参数，被广泛应用于各种寿命试验的数据处理。

55.逆高斯分布 瓦尔德分布
逆高斯分布（Inverse Gaussian distribution）是统计学中一种常用的分布，逆高斯分布起源于有正漂移的Wiener过程或Brown运动中的首达时分布，它与概率论与统计学都有着密切的联系。逆高斯分布在寿命试验、卫生科学、 精算学、生态学、昆虫学等众多领域得到了极为广泛的应用，应当归因于它所具有的十分自然、优良的概率与统计性质。
逆高斯分布（Inverse Gaussian distribution）是统计学中一种常用的分布，其密度函数为
该分布含有两个参数μ和λ ，Wald分布是 μ = λ = 1 时逆高斯分布的特例。当 λ 趋近于无穷时，逆高斯分布逐渐趋近于高斯分布（即正态分布），逆高斯分布有多项类似于高斯分布的特性。“逆”可能容易引起混淆，其实它的含义是高斯分布描述的是在布朗运动中某一固定时刻的距离分布，而逆高斯分布描述的是到达固定距离所需时间的分布。

56.如何产生正太分布数
boxmuller:
Box-Muller，一般是要得到服从正态分布的随机数，基本思想是先得到服从均匀分布的随机数再将服从均匀分布的随机数转变为服从正态分布。
u,v服从[0，1]正太分布
z1=sqrt(-2*log(u))cos(2*PI*v)
z2=sqrt(-2*logu)sin(2*PI*v)


57.1. 学习率η，2. 正则化参数λ，3. 神经网络的层数L，4. 每一个隐层中神经元的个数j，5. 学习的回合数Epoch，6. 小批量数据 minibatch的大小，7. 输出神经元的编码方式，8. 代价函数的选择，9. 权重初始化的方法，10. 神经元激活函数的种类，11.参加训练模型数据的规模 这十一类超参数。


58.网络与输入数据
输入为6为，输出为1维
MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder().seed(12345)
                .l2(0.001).weightInit(WeightInit.XAVIER)
                .updater(new RmsProp(0.1))
                .list()
                .layer(0, new GravesLSTM.Builder().nIn(6).nOut(100).activation(Activation.TANH).build())
                .layer(1, new GravesLSTM.Builder().nIn(lstmLayer1Size).nOut(50).activation(Activation.TANH).build())
                .layer(2, new RnnOutputLayer.Builder(LossFunctions.LossFunction.MSE).activation(Activation.IDENTITY).nIn(50).nOut(OUT_NUM).build())
                .pretrain(false).backprop(true).build();
        MultiLayerNetwork model = new MultiLayerNetwork(conf);
        model.init();
用Nd4j.rand创建1x6x100的训练数据，1为batch，当要创建多个batch时修改他即可；20为一个batch的数据数，100为随机种子，6为数据维度
INDArray input = Nd4j.rand(new int[]{1,6,20}, 100);
为20个数据设置标签，随机种子1000
INDArray label = Nd4j.rand(new int[]{1,1,20}, 1000);
测试数据1个6维向量
INDArray initArray = Nd4j.zeros(1, 6, 1);
把input和label放到DataSet中
DataSet trainingData = new DataSet(input, label);
训练1000个周期
for(int Epochs=0;Epochs<1000;Epochs++){
             model.fit(DataSet);//model.fit(input,label);
             INDArray next = model.rnnTimeStep(initArray);
             System.out.println("next");
             System.out.println(next);
             model.rnnClearPreviousState();
}
注：DL4j做回归最后添加一层网络为
.activationFunction('identity')
.lossFunction(LossFunctions.LossFunction.RMSE)









