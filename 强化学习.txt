1.概念



2.增强学习三大组件
value，policy，model
按照是否包含这些组件来分类：
1）value based
2）policy based
3）actor critic
4）model free
5)model based


3.bellman方程
最优状态值函数 
最优值函数v∗(s)v∗(s)是在所有策略上的最大值函数： 
v∗(s)=maxvπ(s)
v∗(s)=maxvπ(s)
最优行为值函数 
最优行为值函数q∗(s,a)q∗(s,a)是在所有策略上的最大行为值函数： 
q∗(s,a)=maxqπ(s,a)

Bellman最优方程其实就是v∗(s)和q∗(s,a)自身以及相互之间的递推关系。

贝尔曼(Bellman)算法
(又名最短路径算法)
这个算法只适用于没有变化的环境
V(s) = max_{\alpha}(R(s,a)+{\gamma}V({s}'))


4.Qlearning算法
Q一学习是强化学习的主要算法之一，是一种无模型的学习方法，它提供智能系统在马尔可夫环境中利用经历的动作序列选择最优动作的一种学习能力。
Q-学习基于的一个关键假设是智能体和环境的交互可看作为一个Markov决策过程(MDP)，即智能体当前所处的状态和所选择的动作，决定一个固定的状态转移概率分布、下一个状态、并得到一个即时回报。
Q-学习的目标是寻找一个策略可以最大化将来获得的报酬。
Q是怎么训练得来的呢？
Initialize Q arbitrarily //随机初始化Q值
Repeat (for each episode): //每一次游戏，从小鸟出生到死亡是一个episode
    Initialize S //小鸟刚开始飞，S为初始位置的状态
    Repeat (for each step of episode):
        根据当前Q和位置S，使用一种策略，得到动作A //这个策略可以是ε-greedy等
        做了动作A，小鸟到达新的位置S'，并获得奖励R //奖励可以是1，50或者-1000
        Q(S,A) ← (1-α)*Q(S,A) + α*[R + γ*maxQ(S',a)] //在Q中更新S
        S ← S'
    until S is terminal //即到小鸟死亡为止
	
基于神经网络的Q-Learning （Q-Learning with Neural Networks）
对一个简单的网格世界建立一个16*4的表是很容易的，但是在任何一个现在的游戏或真实世界环境中都有无数可能的状态。
对于大多数有趣的问题，表格都无法发挥出作用。因此，我们需要一些代替性的方案来描述我们的状态，
并生成对应动作的Q值：这也就是**神经网络（Neural Network，简称NN）**可以大展身手的地方。NN可以作为一个动作估计器（function approximator），
我们能够输入任意多的可能状态，因为所有状态都可以被编码成一个个向量，并把它们和各自对应的Q值进行对应（map）。



5.Sarsa算法(发音读字母就是了)
Sarsa算法的的决策部分与Qlearning相同，都是通过Q表的形式进行决策，在 Q 表中挑选值较大的动作值施加在环境中来换取奖惩，
也就是根据计算出来的Q值来作为选取动作的依据，两者不同的是行为更新准则是有差异的。Sarsa不会去选取他估计出来的最大Q估计值，而是直接选取估计出来的Q值。
Q-Learning的更新公式
Q∗k+1(s)←∑s'P(s'|s,a)(R(s,a,s')+γmaxa'Q∗(s',a'))
Sarsa的更新公式
Q∗k+1(s)←∑s'P(s'|s,a)(R(s,a,s')+γQ∗(s',a'))
Q(s1, a2) 现实的计算值, 我们也会稍稍改动, 去掉maxQ, 取而代之的是在 s2 上我们实实在在选取的 a2 的 Q 值. 
最后像 Q learning 一样, 求出现实和估计的差距 并更新 Q 表里的 Q(s1, a2)

Sarsa是在线学习(On Policy)的算法，因为他是在行动中学习的，使用了两次greedy方法来选择出了Q(S,A)和q(S',A'）。
而Q learning离线学习(Off Policy)的算法，QLearning选择Q(S,A)用了greedy方法，而计算A(S',A')时用的是max方法，
而真正选择的时候又不一定会选择max的行动。












